# fiap-techchalenge-f2
 Aplicacacao que faz o scrap de dados do site da B3 com dados do pregÃ£o

 ---

## VisÃ£o geral do scraper (`b3_scraper`)

O mÃ³dulo **b3_scraper** automatiza o download diÃ¡rio da *Carteira do Dia* do Ã­ndice **IBovespa** no site da B3 e a transforma em dados estruturados:

1. **Coleta**  
   `scraper.py` usa `http_client.py` (requests + retry) para buscar a pÃ¡gina HTML; o artefato bruto pode ser salvo localmente ou diretamente no Amazon S3 (via `storage.py`) para auditoria/reprocessamento.

2. **Parsing.**  
   `parser.py` extrai a tabela do pregÃ£o (cÃ³digo do papel, nome, tipo, quantidade teÃ³rica, participaÃ§Ã£o %) e converte cada linha em objetos `TradeRecord` (ver `domain/models.py`).

3. **PersistÃªncia estruturada.**  
   Os registros estruturados sÃ£o serializados em **Parquet** e gravados em S3 em `s3://fiap-b3-rawdata/raw/ibov/YYYY/MM/DD/run-.parquet`

Esse particionamento por *ano/mÃªs/dia* Ã© fundamental para custo otimizado de leitura na etapa de analytics.

4. **OrquestraÃ§Ã£o.**  
`application/orchestrator.py` amarra o fluxo *fetch â†’ parse â†’ store* e pode ser executado pelo CLI (`interfaces/cli.py`) ou por contÃªiner/cron.

---

## Objetivos do projeto na AWS 

> **Escopo entregue:** ingestÃ£o & processamento batch completo atÃ© consultas ad-hoc no **Athena**.

| Etapa | ServiÃ§o AWS | O que acontece? |
|-------|-------------|-----------------|
| **IngestÃ£o raw** | **S3** | O scraper grava os Parquets brutos em `raw/ibov/YYYY/MM/DD/`. |
| **Disparo de processamento** | **Lambda (S3 trigger)** | Quando um novo arquivo chega, a Lambda aciona o job Glue visual `bovespa_refined_visual`. |
| **TransformaÃ§Ã£o (camada _refined_)** | **AWS Glue Studio â€“ modo visual** | <br>â‘  **Agrupamento/contagem** por *aÃ§Ã£o* e *data*.<br>â‘¡ **Renomeia** colunas (`stock â†’ acao`, `code â†’ code`, â€¦).<br>â‘¢ Calcula **mÃ©tricas de data** (`days_since_record`, `record_year`, `record_month`).<br><br>O resultado Ã© salvo em Parquet, particionado por `record_date` e `acao_partition`, em `s3://fiap-b3-rawdata/refined/`. |
| **CatÃ¡logo** | **Glue Data Catalog** | O job cria/atualiza a tabela `default.bovespa_refined` apontando para a pasta *refined*. |
| **Consulta ad-hoc** | **Amazon Athena** | Ã‰ possÃ­vel executar SQL diretamente sobre `default.bovespa_refined`, por exemplo:<br>`SELECT * FROM default.bovespa_refined WHERE record_date='2025-07-21' ORDER BY acao;` |


## ğŸ—ï¸ Estrutura do Projeto
```
FIAP-TECHCHALENGE-F2/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt        # dependÃªncias do projeto
â”œâ”€â”€ .env.default           # defaults e placeholders para as configuracoes
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ b3_scraper/             # pacote principal
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py           # carregamento de settings via pydantic
â”‚   â”œâ”€â”€ logger.py           # configuraÃ§Ã£o de logging (rotacionamento, nÃ­veis)
â”‚   â”‚
â”‚   â”œâ”€â”€ domain/             # modelos de negÃ³cio
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ models.py       # dataclasses: TradeRecord, MarketSummary etc.
â”‚   â”‚
â”‚   â”œâ”€â”€ infrastructure/     # â€œexternalsâ€: HTTP, parsing, storage
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ http_client.py  # wrapper requests.Session + retry/backoff
â”‚   â”‚   â”œâ”€â”€ scraper.py      # orquestra fetch de pÃ¡gina(s)
â”‚   â”‚   â”œâ”€â”€ parser.py       # BeautifulSoup â†’ domain.models
â”‚   â”‚   â””â”€â”€ storage.py      # salvar raw HTML / JSON / CSV (local ou S3)
â”‚   â”‚
â”‚   â”œâ”€â”€ application/        # casos de uso
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ orchestrator.py # fluxo: fetch â†’ parse â†’ store
â”‚   â”‚
â”‚   â””â”€â”€ interfaces/         # entry-points
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ cli.py          # argparse para invocar o scraper
â”‚
â””â”€â”€ tests/                  # unitÃ¡rios e de integraÃ§Ã£o
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ test_http_client.py
    â”œâ”€â”€ test_parser.py
    â”œâ”€â”€ test_orchestrator.py
    â””â”€â”€ test_storage.py
```







---

## ğŸ› ï¸ Ambiente local & execuÃ§Ã£o

Siga estes passos para preparar um ambiente **Python 3.9â€Š+** e executar o scraper manualmente:

1. **Clone** o repositÃ³rio (ou baixe o ZIP).  
   ```bash
   git clone https://github.com/nelsonjunior-developer/fiap-techchalenge-f2.git

   cd fiap-techchalenge-f2
   ```

2. **Crie** e **ative** um ambiente virtual:  
   ```bash
   python -m venv .venv
   # Linux / macOS
   source .venv/bin/activate
   # Windows (PowerShell)
   .venv\Scripts\Activate.ps1
   ```

3. **Instale** as dependÃªncias:  
   ```bash
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

4. (Opcional) **Configure variÃ¡veis** de ambiente copiando o template:  
   ```bash
   cp .env.default .env
   # edite .env se precisar alterar chaves / paths
   ```

5. **Execute** o scraper com logging detalhado:  
   ```bash
   python -m b3_scraper.interfaces.cli --verbose
   ```

Se tudo estiver correto, o script farÃ¡ o download da *Carteira do Dia*, parsearÃ¡ os dados e gravarÃ¡ um arquivo Parquet na pasta `raw/` (ou diretamente no S3, dependendo das suas variÃ¡veis de ambiente).